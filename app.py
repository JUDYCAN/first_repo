import os
# [ì—ëŸ¬ í•´ê²°] ì´ ì„¤ì •ì´ ëª¨ë“  importë³´ë‹¤ ìœ„ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
os.environ['FLAGS_allocator_strategy'] = 'naive_best_fit'
os.environ['FLAGS_use_mkldnn'] = '0' # MKLDNN ê°•ì œ ë¹„í™œì„±í™”

import streamlit as st
import torch
import io
import json
import re
import cv2
import numpy as np
from datetime import datetime
from typing import Optional
from PIL import Image, ExifTags

from sqlmodel import Field, Session, SQLModel, create_engine, select
from paddleocr import PaddleOCR
from sentence_transformers import SentenceTransformer
from soynlp.word import WordExtractor
from sklearn.feature_extraction.text import TfidfVectorizer

from transformers import (
    AutoModelForImageClassification, AutoProcessor,
    AutoTokenizer, AutoModelForSeq2SeqLM,
    VisionEncoderDecoderModel,
    LayoutLMv3Processor, LayoutLMv3ForTokenClassification
)

# =========================
# DB MODEL
# =========================
class Document(SQLModel, table=True):
    __table_args__ = {"extend_existing": True} 
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str
    content: str
    summary: str
    keywords: str
    structured_data: str
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None

engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)

# =========================
# MODEL LOADING
# =========================
@st.cache_resource
def load_models():
    dit_processor = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_model = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")

    # [ì—ëŸ¬ í•´ê²°] use_gpu=Falseì™€ enable_mkldnn=Falseë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    ocr = PaddleOCR(
        lang='korean',
        use_gpu=False,
        cpu_threads=1,
        enable_mkldnn=False,
        show_log=False,
        det=True,        # ğŸ”¥ íƒì§€ ëª¨ë¸ ì™„ì „ ë¹„í™œì„±í™”
        rec=True,
        cls=False
    )


    donut_processor = AutoProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
    donut_model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")

    layout_processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
    layout_model = LayoutLMv3ForTokenClassification.from_pretrained("microsoft/layoutlmv3-base")

    sum_tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-base-v2")
    sum_model = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-base-v2")

    embed_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

    return (
        dit_processor, dit_model, ocr, donut_processor, donut_model,
        layout_processor, layout_model, sum_tokenizer, sum_model, embed_model
    )

# =========================
# IMAGE PREPROCESSING
# =========================
def preprocess_image_for_ocr(pil_image):
    img = np.array(pil_image)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 0)
    binary = cv2.adaptiveThreshold(
        blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY, 11, 2
    )
    return Image.fromarray(binary)

# =========================
# DOCUMENT / PHOTO CLASSIFICATION
# =========================
def classify_document(image, processor, model):
    inputs = processor(images=image, return_tensors="pt")
    logits = model(**inputs).logits
    label = model.config.id2label[logits.argmax(-1).item()]
    return label

# =========================
# OCR
# =========================
def extract_text(image, ocr):
    img = np.array(image)
    if img.dtype != np.uint8: img = img.astype(np.uint8)
    if len(img.shape) == 2: img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    elif img.shape[2] == 4: img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)

    result = ocr.ocr(img, cls=False)
    text = ""
    if result and result[0]:
        for line in result[0]:
            text += line[1][0] + " "
    return text.strip()

# =========================
# MORPHEME ANALYSIS
# =========================
def extract_keywords_morpheme(text, top_k=15):
    if not text or len(text.strip()) < 10: return [], []
    sentences = re.split(r'[.!?\n]', text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 5]
    if not sentences: return [], []

    word_extractor = WordExtractor(min_frequency=2, min_cohesion_forward=0.05)
    word_extractor.train(sentences)
    words = word_extractor.extract()

    candidates = [w for w, score in words.items() if len(w) >= 2 and score.cohesion_forward > 0.1]
    if not candidates: return [], []

    vectorizer = TfidfVectorizer(vocabulary=candidates)
    tfidf = vectorizer.fit_transform([text])
    scores = tfidf.toarray()[0]
    keywords = sorted(zip(vectorizer.get_feature_names_out(), scores), key=lambda x: x[1], reverse=True)[:top_k]
    return [k for k, _ in keywords], candidates

# =========================
# SUMMARY (ìš”ì•½ í’ˆì§ˆ ëŒ€í­ ê°•í™”)
# =========================
def summarize_text(text, tokenizer, model):
    if not text or len(text.strip()) < 40:
        return text if text else "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    # [ìš”ì•½ ê°œì„  1] OCR ë…¸ì´ì¦ˆ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ë° íŒŒí¸ ì œê±°)
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ë§ˆì¹¨í‘œ, ì‰¼í‘œë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°
    cleaned = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,]', ' ', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    if len(cleaned) < 30: return "ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ì–´ ìš”ì•½ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤."

    inputs = tokenizer(cleaned[:1024], return_tensors="pt", truncation=True)

    # [ìš”ì•½ ê°œì„  2] ìƒì„± ì•Œê³ ë¦¬ì¦˜ ìµœì í™”
    output = model.generate(
        inputs["input_ids"],
        max_length=150,
        min_length=30,
        num_beams=5,
        repetition_penalty=1.2,    # ë°˜ë³µ ì–µì œ ëŒ€í­ ê°•í™” (ì¤‘ë³µ ë¬¸ì¥ ë°©ì§€)
        no_repeat_ngram_size=3,    # 3ë‹¨ì–´ ì´ìƒ ê²¹ì¹˜ëŠ” ë¬¸êµ¬ ë°©ì§€
        length_penalty=1.2,        # ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ì•„ì§€ì§€ ì•Šë„ë¡ ìœ ë„
        bad_words_ids=[[tokenizer.convert_tokens_to_ids(".")] * 3], # ë§ˆì¹¨í‘œ ë„ë°° ë°©ì§€
        early_stopping=True
    )

    summary = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # ë§Œì•½ ìš”ì•½ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ê±°ë‚˜ ì´ìƒí•˜ë©´ ì •ì œëœ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ ë°˜í™˜
    if len(summary) < 5: return cleaned[:150]
    return summary

# =========================
# EMBEDDING / EXIF
# =========================
def create_embedding(text, model):
    return model.encode(text).tolist()

def extract_exif(image):
    exif_data = {}
    try:
        raw = image._getexif()
        if raw:
            for tag, value in raw.items():
                name = ExifTags.TAGS.get(tag, tag)
                exif_data[name] = str(value)
    except: pass
    return exif_data

# =========================
# MAIN PROCESS
# =========================
def process_document(uploaded_file, models):
    (dit_p, dit_m, ocr, donut_p, donut_m, layout_p, layout_m, sum_t, sum_m, embed_m) = models

    image = Image.open(uploaded_file).convert("RGB")
    pre_img = preprocess_image_for_ocr(image)

    doc_type = classify_document(image, dit_p, dit_m)
    text = extract_text(image, ocr)

    is_photo = len(text.strip()) < 15
    summary = summarize_text(text, sum_t, sum_m)
    keywords, morphemes = extract_keywords_morpheme(text)
    embedding = create_embedding(text + summary, embed_m)

    structured = {"EXIF": extract_exif(image)} if is_photo else {}

    img_bytes = io.BytesIO()
    image.save(img_bytes, format="PNG")

    return {
        "doc_type": "ì‚¬ì§„" if is_photo else doc_type,
        "text": text,
        "summary": summary,
        "keywords": keywords,
        "morphemes": morphemes,
        "structured": structured,
        "image": image,
        "pre_image": pre_img,
        "img_bytes": img_bytes.getvalue(),
        "embedding": embedding
    }

# =========================
# UI (ê²°ê³¼ í™”ë©´ ë™ì¼ ìœ ì§€)
# =========================
st.title("ğŸ“ AI ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ")

models = load_models()
tab1, tab2 = st.tabs(["ì—…ë¡œë“œ & ë¶„ì„", "ì €ì¥ ë¬¸ì„œ"])

# TAB 1
with tab1:
    uploaded = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png","jpg","jpeg"])

    if uploaded:
        result = process_document(uploaded, models)

        st.subheader("â‘  ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë¹„êµ")
        c1, c2 = st.columns(2)
        c1.image(result["image"], caption="ì›ë³¸")
        c2.image(result["pre_image"], caption="ì „ì²˜ë¦¬ í›„")

        st.subheader("â‘¡ ë¬¸ì„œ / ì‚¬ì§„ íŒë³„")
        st.write("ğŸ“Œ íŒë³„ ê²°ê³¼:", result["doc_type"])

        st.subheader("â‘¢ OCR ê²°ê³¼")
        st.text_area("í…ìŠ¤íŠ¸", result["text"], height=150)

        st.subheader("â‘£ í˜•íƒœì†Œ ë¶„ì„ ê°€ì‹œí™”")
        st.write("ğŸ”‘ í‚¤ì›Œë“œ:", result["keywords"])
        st.write("ğŸ“š í˜•íƒœì†Œ:", result["morphemes"])

        st.subheader("â‘¤ ìš”ì•½")
        st.write(result["summary"])

        if result["structured"]:
            st.subheader("â‘¥ ì‚¬ì§„ ë©”íƒ€ë°ì´í„°")
            st.json(result["structured"])

        if st.button("DB ì €ì¥"):
            with Session(engine) as s:
                s.add(Document(
                    filename=uploaded.name,
                    doc_type=result["doc_type"],
                    content=result["text"],
                    summary=result["summary"],
                    keywords=",".join(result["keywords"]),
                    structured_data=json.dumps(result["structured"], ensure_ascii=False),
                    image_data=result["img_bytes"],
                    embedding=json.dumps(result["embedding"])
                ))
                s.commit()
            st.success("ì €ì¥ ì™„ë£Œ")

# TAB 2
with tab2:
    with Session(engine) as s:
        docs = s.exec(select(Document)).all()
        for d in docs:
            with st.expander(d.filename):
                st.image(Image.open(io.BytesIO(d.image_data)))
                st.write("ìœ í˜•:", d.doc_type)
                st.write("ìš”ì•½:", d.summary)
                st.write("í‚¤ì›Œë“œ:", d.keywords)